{"cells":[{"cell_type":"markdown","metadata":{"id":"OTU_sZoLRhgc"},"source":["## READ DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUCQ5Cl58Hcu"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","from sklearn import metrics\n","\n","sns.set_style(\"whitegrid\")\n","\n","def accuracy(target, pred):\n","    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n","\n","def compute_confusion_matrix(target, pred, normalize=None):\n","    return metrics.confusion_matrix(\n","        target.detach().cpu().numpy(), \n","        pred.detach().cpu().numpy(),\n","        normalize=normalize\n","    )\n","\n","def show_image(img):\n","    img = img.detach().cpu()\n","    img = img / 2 + 0.5   # unnormalize\n","    with sns.axes_style(\"white\"):\n","        plt.figure(figsize=(8, 8))\n","        plt.imshow(img.permute((1, 2, 0)).numpy())\n","        plt.axis('off')\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12389,"status":"ok","timestamp":1669828331283,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"zW_E6WOM58Wa","outputId":"90b9c444-6a20-48a5-a9b0-adf86a99a2ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 8.5 MB/s \n","\u001b[?25hInstalling collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n"]}],"source":["pip install pickle5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25791,"status":"ok","timestamp":1669828357069,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"9Lgihs_s_w8c","outputId":"81283f17-01a6-49ec-c2b4-ba064ce27c1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount drive\n","import os.path\n","from os import path\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17334,"status":"ok","timestamp":1669828374396,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"8pG5dkLmx-5l","outputId":"24776620-4085-4a3a-da2c-49aa9af8eb52"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.3.5\n"]}],"source":["import pickle5 as pickle\n","\n","import pandas as pd\n","print(pd.__version__)\n","\n","path = \"/content/drive/My Drive/ColabNotebooks/projekt03/data_10000_images_v_135.pkl\" #Directory Frederikke\n","#path = \"/content/drive/My Drive/projekt03/data_10000_images_v_135.pkl\" # Directory Jonathan\n","#path = \"/content/drive/My Drive/ColabNotebooks/projekt03/data_10000_images_v_135.pkl\" # Directory Johanne\n","\n","with open(path, \"rb\") as fh:\n","  subset = pickle.load(fh)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1669828374397,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"KyMsBlpLgI51","outputId":"9b885c1a-774f-4202-d83f-5bed6abc08bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype('int64')"]},"metadata":{},"execution_count":5}],"source":["#convet labels into int\n","labels_names = subset.moa.unique()\n","label_to_label_id = {}\n","for i in range(len(labels_names)):\n","    label_to_label_id[labels_names[i]] = i\n","\n","subset[\"labels_ID\"] = subset.apply(lambda row: label_to_label_id[row.moa], axis=1)\n","subset[\"labels_ID\"].dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669828374397,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"Ur4hrBrVAq5i","outputId":"e758b2b9-b09f-4567-b800-d2441455f3e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10010, 2)\n"]}],"source":["#make new dataframe that dataloader can read\n","data = subset[['Images','labels_ID']].copy()\n","print(data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C99UVJfyRRSC"},"outputs":[],"source":["#make dataset class\n","#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","from torchvision.io import read_image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","class BBBC(Dataset):\n","    def __init__(self, annotations_file,typeFlag, transform=None, target_transform=None):\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        \n","        train_size = int(np.ceil(0.8*annotations_file.shape[0]))\n","        test_size = int(annotations_file.shape[0]-train_size)\n","\n","        if typeFlag == 'train':\n","          img_labels = annotations_file.head(train_size)\n","        elif typeFlag == 'test':\n","          img_labels = annotations_file.head(-test_size)\n","        \n","        self.img_labels = img_labels\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.img_labels.iloc[idx, 0]\n","        \n","        label = self.img_labels.iloc[idx, 1]\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        \n","        image = image.view(-1).type(torch.FloatTensor)\n","        image = (image-image.min())/(image.max()-image.min())\n","        image = image.view(3,68,68).type(torch.FloatTensor)\n","\n","        return image, int(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPfXi914DFfe"},"outputs":[],"source":["#Read data with dataloader\n","\n","from torch.utils.data import DataLoader\n","\n","data = data.sample(frac=2002/len(data)).reset_index(drop=True)\n","\n","n_classes = len(set(data[\"labels_ID\"]))\n","\n","dset_train = BBBC(data,\"train\")\n","dset_test = BBBC(data,\"test\")\n","\n","batch_size = 128\n","\n","train_loader = DataLoader(dset_train,batch_size=batch_size)\n","test_loader = DataLoader(dset_test,batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1669828375275,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"G7HjyBVZ9inU","outputId":"518d22d9-9e7f-4c54-978c-6b3ff47c498b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch dimension (B x C x H x W): torch.Size([128, 3, 68, 68])\n"]}],"source":["x, y = next(iter(train_loader))\n","print(\"Batch dimension (B x C x H x W):\", x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669828417034,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"-G4ilCGY7Wm7","outputId":"c38193ae-6b1a-4575-e144-edea4d8e1e36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (net): Sequential(\n","    (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.1, inplace=False)\n","    (4): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): ReLU()\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Dropout(p=0.1, inplace=False)\n","    (8): Conv2d(12, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (12): Flatten(start_dim=1, end_dim=-1)\n","    (13): Linear(in_features=16384, out_features=13, bias=True)\n","  )\n",")\n"]}],"source":["class PrintSize(nn.Module):\n","    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first pass.\"\"\"\n","    \n","    first = True\n","    \n","    def forward(self, x):\n","        if self.first:\n","            print(f\"Size: {x.size()}\")\n","            self.first = False\n","        return x\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, num_classes,):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        \n","        #your code here\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3,12,3,padding=1),\n","            nn.BatchNorm2d(12),\n","            nn.ReLU(),\n","            torch.nn.Dropout(0.1),\n","            nn.Conv2d(12,12,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            torch.nn.Dropout(0.1),\n","            nn.Conv2d(12, 64, 3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            nn.Flatten(1,-1),\n","\n","            nn.Linear(in_features=16384, out_features=num_classes)\n","\n","            #nn.Conv2d(3, 16,3, stride=1, padding = 1),\n","            #nn.ReLU(),\n","\n","            #nn.Conv2d(16, 32,3, stride=1, padding = 1),\n","            #nn.MaxPool2d(2, stride=2),\n","            #nn.ReLU(),\n","\n","            #nn.Conv2d(32,64,3, stride=1, padding = 1),\n","            #nn.MaxPool2d(2, stride=2),\n","            #nn.ReLU(),\n","\n","            #nn.Flatten(1,-1),\n","            #nn.Linear(17*17*64, 100),\n","            #nn.ReLU(),\n","            #nn.Linear(100, num_classes))\n","            )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","model = Model(n_classes)\n","device = torch.device('cpu')  # use cuda or cpu\n","model.to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EyusbQG8SUK"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669828424740,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"},"user_tz":-60},"id":"3xwNYnUe8X01","outputId":"6515c0ef-ae7b-4057-e548-0e8f108ad173"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([2, 13])\n","Output logits:\n","[[-0.4928476  -1.132153    0.8413316  -0.0532299   0.5823067   0.06716603\n","  -0.7126588   0.5920632   1.0905435   0.8280716   0.86114055  0.3057353\n","  -0.5799852 ]\n"," [-0.34384996 -1.6320821   0.9210721  -0.54154235  0.23263094 -0.6963913\n","  -1.3192508  -0.4157709   0.7289497   0.97881067  0.93925965  0.95906085\n","  -1.2883418 ]]\n","Output probabilities:\n","[[0.03231046 0.0170489  0.12267888 0.0501495  0.09468402 0.05656581\n","  0.02593464 0.09561233 0.1573987  0.1210629  0.12513325 0.07180642\n","  0.02961418]\n"," [0.04204654 0.01159467 0.14896321 0.03450433 0.07483294 0.0295545\n","  0.01585328 0.0391287  0.12292531 0.15781727 0.15169726 0.15473099\n","  0.01635094]]\n"]}],"source":["# Test the forward pass with dummy data\n","out = model(torch.randn(2, 3, 68, 68, device=device))\n","print(\"Output shape:\", out.size())\n","print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")\n","print(f\"Output probabilities:\\n{out.softmax(1).detach().cpu().numpy()}\")"]},{"cell_type":"code","source":["batch_size = 128\n","num_epochs = 10\n","validation_every_steps = 500\n","\n","step = 0\n","model.train()\n","\n","train_accuracies = []\n","valid_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    print(epoch)\n","    train_accuracies_batches = []\n","    \n","    for inputs, targets in train_loader:\n","        #inputs, targets = inputs.to(device), targets.to(device)\n","        inputs, targets = next(iter(train_loader))\n","\n","        # Forward pass, compute gradients, perform one training step.\n","        # Your code here!\n","        # Forward pass.\n","        output = model(inputs)\n","        \n","        # Compute loss.\n","        loss = loss_fn(output, targets)\n","        \n","        # Clean up gradients from the model.\n","        optimizer.zero_grad()\n","        \n","        # Compute gradients based on the loss from the current batch (backpropagation).\n","        loss.backward()\n","        \n","        # Take one optimizer step using the gradients computed in the previous step.\n","        optimizer.step()\n","        \n","        step += 1\n","\n","        # Increment step counter\n","        step += 1\n","        \n","        # Compute accuracy.\n","        predictions = output.max(1)[1]\n","        train_accuracies_batches.append(accuracy(targets, predictions))\n","        \n","        if step % validation_every_steps == 0:\n","            \n","            # Append average training accuracy to list.\n","            train_accuracies.append(np.mean(train_accuracies_batches))\n","            \n","            train_accuracies_batches = []\n","        \n","            # Compute accuracies on validation set.\n","            valid_accuracies_batches = []\n","            with torch.no_grad():\n","                model.eval()\n","                for inputs, targets in test_loader:\n","                    inputs, targets = inputs.to(device), targets.to(device)\n","                    output = model(inputs)\n","                    loss = loss_fn(output, targets)\n","\n","                    predictions = output.max(1)[1]\n","\n","                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n","                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n","\n","                model.train()\n","                \n","            # Append average validation accuracy to list.\n","            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(dset_test))\n","     \n","            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n","            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n","\n","print(\"Finished training.\")"],"metadata":{"id":"KxFwosq0IgI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0hlGk7cFmna"},"outputs":[],"source":["# Evaluate test set\n","confusion_matrix = np.zeros((n_classes, n_classes))\n","with torch.no_grad():\n","    model.eval()\n","    test_accuracies = []\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        output = model(inputs)\n","        loss = loss_fn(output, targets)\n","\n","        predictions = output.max(1)[1]\n","\n","        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n","        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n","        \n","        confusion_matrix += compute_confusion_matrix(targets, predictions)\n","\n","    test_accuracy = np.sum(test_accuracies) / len(dset_test)\n","    \n","    model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ztCaKlEICZT"},"outputs":[],"source":["print(f\"Test accuracy: {test_accuracy:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YrZnpQQwLRfp","executionInfo":{"status":"error","timestamp":1669828975079,"user_tz":-60,"elapsed":479303,"user":{"displayName":"Frederikke Uldahl","userId":"02184195015383787305"}},"outputId":"e4b1c199-e0f1-467a-98ab-52f334dd8541"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model 0: 0/50\n","Model 0: 1/50\n","Model 0: 2/50\n","Model 0: 3/50\n","Model 0: 4/50\n","Model 0: 5/50\n","Model 0: 6/50\n","Model 0: 7/50\n","Model 0: 8/50\n","Model 0: 9/50\n","Model 0: 10/50\n","Model 0: 11/50\n","Model 0: 12/50\n","Model 0: 13/50\n","Model 0: 14/50\n","Model 0: 15/50\n","Model 0: 16/50\n","Model 0: 17/50\n","Model 0: 18/50\n","Model 0: 19/50\n","Step 500     training accuracy: 1.0\n","             test accuracy: 0.2222222222222222\n","Model 0: 20/50\n","Model 0: 21/50\n","Model 0: 22/50\n","Model 0: 23/50\n","Model 0: 24/50\n","Model 0: 25/50\n","Model 0: 26/50\n","Model 0: 27/50\n","Model 0: 28/50\n","Model 0: 29/50\n","Model 0: 30/50\n","Model 0: 31/50\n","Model 0: 32/50\n","Model 0: 33/50\n","Model 0: 34/50\n","Model 0: 35/50\n","Model 0: 36/50\n","Model 0: 37/50\n","Model 0: 38/50\n","Step 1000    training accuracy: 1.0\n","             test accuracy: 0.22534332084893882\n","Model 0: 39/50\n","Model 0: 40/50\n","Model 0: 41/50\n","Model 0: 42/50\n","Model 0: 43/50\n","Model 0: 44/50\n","Model 0: 45/50\n","Model 0: 46/50\n","Model 0: 47/50\n","Model 0: 48/50\n","Model 0: 49/50\n","Finished training.\n","run0: 0.22846441947565543\n","Model 1: 0/50\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-0d7e794a160f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                   \u001b[0;31m# Compute gradients based on the loss from the current batch (backpropagation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                   \u001b[0;31m# Take one optimizer step using the gradients computed in the previous step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","test_accuracys = []\n","for i in range(10):\n","          #Read data with dataloader\n","\n","          from torch.utils.data import DataLoader\n","\n","          data = data.sample(frac=2002/len(data)).reset_index(drop=True)\n","\n","          n_classes = len(set(data[\"labels_ID\"]))\n","\n","          dset_train = BBBC(data,\"train\")\n","          dset_test = BBBC(data,\"test\")\n","\n","          batch_size = 128\n","\n","          train_loader = DataLoader(dset_train,batch_size=batch_size)\n","          test_loader = DataLoader(dset_test,batch_size=batch_size)\n","\n","          model = Model(n_classes)\n","          device = torch.device('cpu')  # use cuda or cpu\n","          model.to(device)\n","\n","          loss_fn = nn.CrossEntropyLoss()\n","          optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","          batch_size = 128\n","          num_epochs = 50\n","          validation_every_steps = 500\n","\n","          step = 0\n","          model.train()\n","\n","          train_accuracies = []\n","          valid_accuracies = []\n","\n","          for epoch in range(num_epochs):\n","              print(f\"Model {i}: {epoch}/{num_epochs}\")\n","              train_accuracies_batches = []\n","              \n","              for inputs, targets in train_loader:\n","                  #inputs, targets = inputs.to(device), targets.to(device)\n","                  inputs, targets = next(iter(train_loader))\n","\n","                  # Forward pass, compute gradients, perform one training step.\n","                  # Your code here!\n","                  # Forward pass.\n","                  output = model(inputs)\n","                  \n","                  # Compute loss.\n","                  loss = loss_fn(output, targets)\n","                  \n","                  # Clean up gradients from the model.\n","                  optimizer.zero_grad()\n","                  \n","                  # Compute gradients based on the loss from the current batch (backpropagation).\n","                  loss.backward()\n","                  \n","                  # Take one optimizer step using the gradients computed in the previous step.\n","                  optimizer.step()\n","                  \n","                  step += 1\n","\n","                  # Increment step counter\n","                  step += 1\n","                  \n","                  # Compute accuracy.\n","                  predictions = output.max(1)[1]\n","                  train_accuracies_batches.append(accuracy(targets, predictions))\n","                  \n","                  if step % validation_every_steps == 0:\n","                      \n","                      # Append average training accuracy to list.\n","                      train_accuracies.append(np.mean(train_accuracies_batches))\n","                      \n","                      train_accuracies_batches = []\n","                  \n","                      # Compute accuracies on validation set.\n","                      valid_accuracies_batches = []\n","                      with torch.no_grad():\n","                          model.eval()\n","                          for inputs, targets in test_loader:\n","                              inputs, targets = inputs.to(device), targets.to(device)\n","                              output = model(inputs)\n","                              loss = loss_fn(output, targets)\n","\n","                              predictions = output.max(1)[1]\n","\n","                              # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n","                              valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n","\n","                          model.train()\n","                          \n","                      # Append average validation accuracy to list.\n","                      valid_accuracies.append(np.sum(valid_accuracies_batches) / len(dset_test))\n","              \n","                      print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n","                      print(f\"             test accuracy: {valid_accuracies[-1]}\")\n","\n","          print(\"Finished training.\")\n","\n","          # Evaluate test set\n","          confusion_matrix = np.zeros((n_classes, n_classes))\n","          with torch.no_grad():\n","              model.eval()\n","              test_accuracies = []\n","              for inputs, targets in test_loader:\n","                  inputs, targets = inputs.to(device), targets.to(device)\n","                  output = model(inputs)\n","                  loss = loss_fn(output, targets)\n","\n","                  predictions = output.max(1)[1]\n","\n","                  # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).\n","                  test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n","                  \n","                  confusion_matrix += compute_confusion_matrix(targets, predictions)\n","\n","              test_accuracy = np.sum(test_accuracies) / len(dset_test)\n","              \n","              model.train()\n","              print(f\"run{i}: {test_accuracy}\")\n","              test_accuracys.append(test_accuracy)\n","  \n","print(test_accuracys)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1mxv6e0Od5Kv0tRhgAEWZCazLUQfY9oov","timestamp":1669626510867}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}